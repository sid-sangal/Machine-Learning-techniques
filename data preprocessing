We utilize the pandas library for importing the dataset into our current instance of google colab.

1. dataset = pd.read_csv('name of the file like data.csv')

For performing any sort of predictive analysis on supervised learning data we need an input matrix 'X' and a label matrix'Y'
X = dataset.iloc(rows,colums) #the iloc function locates the indices i. We specify the range of the inputs from the csv file that we want to include in the X matrix

We define the indices of the columns.
X = dataset.iloc(:,:-1) # we take all the rwos from the dataset and the columns from the first to the second last one.
Y = dataset.iloc(:,-1) # we take all rows and only the last column



Handling Missing data:

There are many ways to consider missing data. For this instance we have integer values in the column. For a dataset where the values are not very far apart or does not contain
outliers, taking a mean of all values and replacing them works. This is suceptibe to outliers and hence a median or mode should be used in this case depending upon the effect of
left or right skew on the dataset.

Use sklearn to handle missing data

from sklearn.impute import SimpleImputer

Create an instance of the imported library

imputer = SimpleImputer(missing_values = np.nan, strategy = mean) # we use the numpy library to find the input values that are empty and are denoted by nan in python.

imputer.fit(X[:,1:3]) # this find all missing values in the X matrix for all rows and the columns 1,2 and excludes 3.

X[:,1,3] = impute,transform(X[:,1,3])

#print the above transformation and check wherether it has no nan values.

Encoding categorical data essentially means converting string values to numerical values depending on the number of classes for graphical represntation and ease of calculation.
# The method that will be explored today is one hot encoding

For n classes, we represent the classes by n - bit binary representation or vector notation. 

Dependant libraries:

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

We create an instance and then pass the X matrix into it . There are specific inbuilt functions that can be googled easily when needed.

For binary (Yes/No) label in the 'y' matrix, we utilize another sklearn library

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(y)
print(y) # this will tranform the yes nad no labels into 1 and 0 values which are necessary for performing computation of any kind.

*Important*
Use feature scaling after splitting the dataset into train and test.

Explanation: using feature scaling involves using the standard deviation and mean of the dataset. If we use the entire datset, that would create a bias at it would use the data from the test set as well.

For splitting the entire dataset into train and test:

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(# this will take 4 inputs. The x matrix, the y matrix, the ratio of the test set size and the random_state which is a binary variable)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)

Feature scaling:

We dont use it everytime. We use feature scaling for smoothing the data. We scale the values linearly such as bringing all the values int othe range of [0,1] for ease of calculation and understanding

Standardizatio and Normalizaton are two techniques.
For non Neural Net techniques, there size of the batch of the dataset is the entire dataset. For NN there exists a version of this called batch normalization.

Additional resources:
https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35?gi=3ff6aaec3ce
https://www.youtube.com/watch?v=mnKm3YP56PY

There is a separate file focusing on the techniques

